{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor critic (part1: 对局数据收集)\n",
    " \n",
    "    agent1 and agent2 are identical\n",
    "    combine their experiences for training (从两个方便收集到的数据一块训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_rl/go\n",
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "\n",
    "from dlgo import agent\n",
    "from dlgo import scoring\n",
    "from dlgo import rl\n",
    "from dlgo.goboard_fast import GameState, Player, Point\n",
    "\n",
    "from dlgo.encoders import get_encoder_by_name\n",
    "from dlgo.networks import acnet_small\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 脚本输入参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据 模型存放目录\n",
    "data_home_path = 'data/ac/'\n",
    "if not os.path.exists(data_home_path):\n",
    "    os.makedirs(data_home_path)\n",
    "\n",
    "# 脚本输入参数 \n",
    "class args:\n",
    "    board_size = 9 # 缩小计算量, 保证算法的验证速度\n",
    "    num_games = 10 # 每轮迭代只收集10games的数据\n",
    "    learning_agent = data_home_path + 'agent_checkpoint.pth'\n",
    "    experience_out = data_home_path + 'experience.pth'\n",
    "    \n",
    "print(os.path.exists(args.learning_agent))\n",
    "\n",
    "# 全局变量\n",
    "global BOARD_SIZE\n",
    "BOARD_SIZE = args.board_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = 'ABCDEFGHJKLMNOPQRST'\n",
    "STONE_TO_CHAR = {\n",
    "    None: '.',\n",
    "    Player.black: 'x',\n",
    "    Player.white: 'o',\n",
    "}\n",
    "\n",
    "def avg(items):\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    return sum(items) / float(len(items))\n",
    "\n",
    "\n",
    "def print_board(board):\n",
    "    for row in range(BOARD_SIZE, 0, -1):\n",
    "        line = []\n",
    "        for col in range(1, BOARD_SIZE + 1):\n",
    "            stone = board.get(Point(row=row, col=col))\n",
    "            line.append(STONE_TO_CHAR[stone])\n",
    "        print('%2d %s' % (row, ''.join(line)))\n",
    "    print('   ' + COLS[:BOARD_SIZE])\n",
    "\n",
    "\n",
    "class GameRecord(namedtuple('GameRecord', 'moves winner margin')):\n",
    "    pass\n",
    "\n",
    "def name(player):\n",
    "    if player == Player.black:\n",
    "        return 'B'\n",
    "    return 'W'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟对局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_game(black_player, white_player):\n",
    "    moves = []\n",
    "    game = GameState.new_game(BOARD_SIZE)\n",
    "    agents = {\n",
    "        Player.black: black_player,\n",
    "        Player.white: white_player,\n",
    "    }\n",
    "    while not game.is_over():\n",
    "        next_move = agents[game.next_player].select_move(game)\n",
    "        moves.append(next_move)\n",
    "        game = game.apply_move(next_move)\n",
    "\n",
    "    print_board(game.board)\n",
    "    game_result = scoring.compute_game_result(game)\n",
    "    print(game_result)\n",
    "\n",
    "    # nametuple todo moves作用？margin作用？\n",
    "    return GameRecord(\n",
    "        moves=moves,\n",
    "        winner=game_result.winner,\n",
    "        margin=game_result.winning_margin,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop of self_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "    agent_filename = args.learning_agent\n",
    "    experience_filename = args.experience_out\n",
    "    num_games = args.num_games\n",
    "\n",
    "    # init agent object\n",
    "    encoder_name = 'sevenplane'\n",
    "    model = acnet_small(input_channel_num=7, board_size=BOARD_SIZE)\n",
    "\n",
    "    agent1, agent2 = None, None\n",
    "    if not os.path.exists(agent_filename): # check_point不存在\n",
    "        encoder = get_encoder_by_name(name=encoder_name, board_size=BOARD_SIZE)\n",
    "        agent1 = rl.load_ac_agent(model=model, encoder=encoder, device=device)\n",
    "        agent2 = rl.load_ac_agent(model=model, encoder=encoder, device=device)\n",
    "    else: \n",
    "        # 指向同一个model没啥问题吧, 变量重新初始化了一遍\n",
    "        agent1 = rl.load_ac_agent(model=model, save_path=agent_filename, device=device)\n",
    "        agent2 = rl.load_ac_agent(model=model, save_path=agent_filename, device=device)\n",
    "    assert (agent1 is not None) and (agent2 is not None)\n",
    "\n",
    "    collector1 = rl.ExperienceCollector()\n",
    "    collector2 = rl.ExperienceCollector()\n",
    "    agent1.set_collector(collector1)\n",
    "    agent2.set_collector(collector2)\n",
    "    #######################################################################\n",
    "    t1 = time.time()\n",
    "    for i in range(args.num_games):\n",
    "        print('Simulating game %d/%d...' % (i + 1, args.num_games))\n",
    "        collector1.begin_episode()\n",
    "        collector2.begin_episode()\n",
    "\n",
    "        game_record = simulate_game(agent1, agent2)\n",
    "        if game_record.winner == Player.black:\n",
    "            collector1.complete_episode(reward=1)\n",
    "            collector2.complete_episode(reward=-1)\n",
    "        else:\n",
    "            collector2.complete_episode(reward=1)\n",
    "            collector1.complete_episode(reward=-1)\n",
    "\n",
    "    print(\"simulatinon of {} games , cost_time:{:.3f}s.\".format(args.num_games, time.time()-t1))\n",
    "    \n",
    "    experience = rl.combine_experience([collector1, collector2]) # 整合所有训练数据\n",
    "    experience.serialize(experience_filename) # 序列化存储\n",
    "    print(\"collect {} samples\".format(len(experience)))\n",
    "    \n",
    "main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
