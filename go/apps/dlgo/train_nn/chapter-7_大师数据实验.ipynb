{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习，预测next_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_rl\n"
     ]
    }
   ],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_rl/go\n",
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dlgo.data import GoDataset\n",
    "from dlgo.networks.resnet import resnet34, resnet18\n",
    "from dlgo.networks.small import CnnSmall\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "datatype = torch.float32\n",
    "# datatype = torch.float16 # 数据容易溢出？？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "    生成数据，先执行apps/dlgo/data/data_prepare_pipline.ipynb\n",
    "    然后修改下面的preprocess_dir目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388096 12288\n",
      "torch.Size([128, 7, 19, 19]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "preprocess_dir = 'data/sevenplane/'\n",
    "num_workers = 0\n",
    "train_dataset = GoDataset(preprocess_dir, datatype='train', device=device)\n",
    "test_dataset = GoDataset(preprocess_dir, datatype='test', device=device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "for x,y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数等\n",
    "    训练集合总共 977920条样本， 测试集合190464条样本\n",
    "    batch size 64, 0.4s 一个batch的迭代, 一个epoch 需要2h\n",
    "    batch size 128, 0.9s 一个batch的迭代, 一个epoch 需要2h\n",
    "    batch size 1024, 15s 一个batch的迭代, 一个epoch 需要4h\n",
    "\n",
    "    改为resnet18后，\n",
    "    batch size 128, 0.4s 一个batch的迭代, 一个epoch 需要1h\n",
    "    进一步改小resnet18\n",
    "    batch size 128, 0.2s 一个batch的迭代, 一个epoch 需要0.5h\n",
    "    batch size 256, 0.5s 一个batch的迭代, 一个epoch 需要0.6h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不是很好的 chapter-6模型\n",
    "# cnn_model = nn.Sequential(*[\n",
    "#     nn.Conv2d(1, 48, kernel_size=3, padding=0, stride=1, bias=False),\n",
    "#     nn.BatchNorm2d(48),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "    \n",
    "#     nn.Conv2d(48, 48, kernel_size=3, padding=0, stride=1, bias=False),\n",
    "#     nn.BatchNorm2d(48),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2,),\n",
    "#     nn.Dropout(p=0.5),\n",
    "    \n",
    "#     nn.Flatten(start_dim=1),\n",
    "#     nn.Linear(48*7*7, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.Linear(1024, 19*19),\n",
    "#     ])\n",
    "\n",
    "# x = torch.randn(64,1, 19, 19)\n",
    "# for layer in cnn_model:\n",
    "#     print(layer)\n",
    "#     x= layer(x)\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(7, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 64, 19, 19])\n",
      "Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 64, 19, 19])\n",
      "Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 64, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 64, 19, 19])\n",
      "Conv2d(64, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 48, 19, 19])\n",
      "Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 48, 19, 19])\n",
      "Conv2d(48, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 32, 19, 19])\n",
      "Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 32, 19, 19])\n",
      "Flatten(start_dim=1, end_dim=-1)\n",
      "torch.Size([64, 11552])\n",
      "Linear(in_features=11552, out_features=1024, bias=True)\n",
      "torch.Size([64, 1024])\n",
      "ReLU()\n",
      "torch.Size([64, 1024])\n",
      "Linear(in_features=1024, out_features=361, bias=True)\n",
      "torch.Size([64, 361])\n"
     ]
    }
   ],
   "source": [
    "cnn_large_model = nn.Sequential(*[\n",
    "    nn.Conv2d(7, 64, kernel_size=7, padding=3, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(64, 48, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(48),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(48, 48, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(48),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(48, 32, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(32*19*19, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 19*19),\n",
    "    ])\n",
    "\n",
    "x = torch.randn(64, 7, 19, 19)\n",
    "for layer in cnn_large_model:\n",
    "    print(layer)\n",
    "    x= layer(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(7, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 48, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 48, 19, 19])\n",
      "Conv2d(48, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 32, 19, 19])\n",
      "Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 32, 19, 19])\n",
      "Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([64, 32, 19, 19])\n",
      "ReLU()\n",
      "torch.Size([64, 32, 19, 19])\n",
      "Flatten(start_dim=1, end_dim=-1)\n",
      "torch.Size([64, 11552])\n",
      "Linear(in_features=11552, out_features=512, bias=True)\n",
      "torch.Size([64, 512])\n",
      "ReLU()\n",
      "torch.Size([64, 512])\n",
      "Linear(in_features=512, out_features=361, bias=True)\n",
      "torch.Size([64, 361])\n"
     ]
    }
   ],
   "source": [
    "cnn_small_model = nn.Sequential(*[\n",
    "    nn.Conv2d(7, 48, kernel_size=7, padding=3, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(48),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(48, 32, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(32, 32, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv2d(32, 32, kernel_size=5, padding=2, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(32*19*19, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 19*19),\n",
    "    ])\n",
    "\n",
    "x = torch.randn(64, 7, 19, 19)\n",
    "for layer in cnn_small_model:\n",
    "    print(layer)\n",
    "    x= layer(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    n_test = len(test_dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        succ_num = 0\n",
    "        for x, label in test_dataloader:\n",
    "            y = model(x)\n",
    "            # y = torch.randn((x.shape[0], 81), device=device, dtype=datatype) # 测试随机有1.25%的正确率\n",
    "            # 计算预测正确概率\n",
    "            # print(type(y), y.shape, y)\n",
    "            predict = torch.argmax(y, dim=1)\n",
    "            # print(type(predict), predict.shape, predict)\n",
    "            # print(type(label),label.shape, label)\n",
    "            succ_num += torch.sum(predict == label)\n",
    "        return succ_num/n_test # 预测正确的概率 [瞎猜的概率1/9*9 = 12%]\n",
    "\n",
    "def train(train_dataloader, model, loss_fn, optimizer, scheduler=None, epochs=1, test_dataloader=None):    \n",
    "    n_train = len(train_dataloader.dataset) # 总样本量\n",
    "    best_test_pred = 0\n",
    "    best_epoch = -1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        acc_loss = 0\n",
    "        i = 0\n",
    "        train_num = 0\n",
    "        train_succ_num = 0\n",
    "        \n",
    "        for x, label in train_dataloader:\n",
    "            y = model(x)\n",
    "            loss = loss_fn(y, label)\n",
    "            # print(y.shape,y)\n",
    "            # print(loss.shape, loss.item())\n",
    "            # break\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 预测数据统计\n",
    "            train_succ_num += torch.sum(torch.argmax(y, dim=1) == label)\n",
    "            train_num += x.shape[0]\n",
    "            acc_loss += loss.item()\n",
    "            i += 1\n",
    "            \n",
    "            # batch日志输出\n",
    "            if i%200 == 0:\n",
    "                print(\"epoch:{}, i:{}/{}, avg_loss:{:.3f}, avg_succ{:.3f}%\"\n",
    "                      .format(epoch, i,\n",
    "                              n_train//x.shape[0],\n",
    "                              acc_loss/train_num,\n",
    "                              train_succ_num / train_num * 100,\n",
    "                              ))\n",
    "        if scheduler: # 每个epoch更新一次\n",
    "            scheduler.step()\n",
    "        #########  end epoch  ###########\n",
    "        print(\"-----------------------------\")\n",
    "        # 训练集的准确率\n",
    "        print(\"Epoch {}: train_pred_succ:{:.3f}%, {}\".format(epoch, train_succ_num / train_num * 100, train_num))\n",
    "        \n",
    "        if test_dataloader:\n",
    "            n_test = len(test_dataloader.dataset) \n",
    "            test_succ_ratio = evaluate(model, test_dataloader) * 100\n",
    "            \n",
    "            if test_succ_ratio > best_test_pred:\n",
    "                best_test_pred = test_succ_ratio\n",
    "                best_epoch = epoch \n",
    "            \n",
    "            print(\"Epoch {}: test_pred_succ:{:.3f}%, {}\".format(epoch, test_succ_ratio, n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(epoch))\n",
    "            \n",
    "        print('\\n')\n",
    "\n",
    "    # 训练结束\n",
    "    print(\"best_epoch:{}, best_test_pred:{:.3f}%\".format(best_epoch, best_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练记录\n",
    "\n",
    "200局游戏，100局验证  best_test_pred:1.839%， 迭代了几十轮\n",
    "2000局游戏，100局验证 ，5轮epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet18(num_classes=19*19, encoder_planes=7).to(datatype).to(device)\n",
    "# model = cnn_model.to(datatype).to(device)\n",
    "# model = cnn_small_model.to(datatype).to(device)\n",
    "model = cnn_large_model.to(datatype).to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, i:200/3032, avg_loss:0.045, avg_succ0.410%\n",
      "epoch:0, i:400/3032, avg_loss:0.045, avg_succ0.479%\n",
      "epoch:0, i:600/3032, avg_loss:0.045, avg_succ0.561%\n",
      "epoch:0, i:800/3032, avg_loss:0.045, avg_succ0.691%\n",
      "epoch:0, i:1000/3032, avg_loss:0.044, avg_succ0.784%\n",
      "epoch:0, i:1200/3032, avg_loss:0.044, avg_succ0.929%\n",
      "epoch:0, i:1400/3032, avg_loss:0.044, avg_succ1.053%\n",
      "epoch:0, i:1600/3032, avg_loss:0.043, avg_succ1.176%\n",
      "epoch:0, i:1800/3032, avg_loss:0.043, avg_succ1.309%\n",
      "epoch:0, i:2000/3032, avg_loss:0.043, avg_succ1.446%\n",
      "epoch:0, i:2200/3032, avg_loss:0.042, avg_succ1.589%\n",
      "epoch:0, i:2400/3032, avg_loss:0.042, avg_succ1.714%\n",
      "epoch:0, i:2600/3032, avg_loss:0.042, avg_succ1.860%\n",
      "epoch:0, i:2800/3032, avg_loss:0.042, avg_succ1.982%\n",
      "epoch:0, i:3000/3032, avg_loss:0.042, avg_succ2.105%\n",
      "-----------------------------\n",
      "Epoch 0: train_pred_succ:2.118%, 388096\n",
      "Epoch 0: test_pred_succ:4.069%, 12288\n",
      "\n",
      "\n",
      "epoch:1, i:200/3032, avg_loss:0.039, avg_succ4.234%\n",
      "epoch:1, i:400/3032, avg_loss:0.038, avg_succ4.365%\n",
      "epoch:1, i:600/3032, avg_loss:0.038, avg_succ4.483%\n",
      "epoch:1, i:800/3032, avg_loss:0.038, avg_succ4.666%\n",
      "epoch:1, i:1000/3032, avg_loss:0.038, avg_succ4.824%\n",
      "epoch:1, i:1200/3032, avg_loss:0.038, avg_succ4.975%\n",
      "epoch:1, i:1400/3032, avg_loss:0.037, avg_succ5.160%\n",
      "epoch:1, i:1600/3032, avg_loss:0.037, avg_succ5.328%\n",
      "epoch:1, i:1800/3032, avg_loss:0.037, avg_succ5.554%\n",
      "epoch:1, i:2000/3032, avg_loss:0.037, avg_succ5.776%\n",
      "epoch:1, i:2200/3032, avg_loss:0.037, avg_succ6.021%\n",
      "epoch:1, i:2400/3032, avg_loss:0.037, avg_succ6.290%\n",
      "epoch:1, i:2600/3032, avg_loss:0.037, avg_succ6.593%\n",
      "epoch:1, i:2800/3032, avg_loss:0.036, avg_succ6.864%\n",
      "epoch:1, i:3000/3032, avg_loss:0.036, avg_succ7.170%\n",
      "-----------------------------\n",
      "Epoch 1: train_pred_succ:7.219%, 388096\n",
      "Epoch 1: test_pred_succ:11.507%, 12288\n",
      "\n",
      "\n",
      "epoch:2, i:200/3032, avg_loss:0.034, avg_succ12.336%\n",
      "epoch:2, i:400/3032, avg_loss:0.034, avg_succ12.736%\n",
      "epoch:2, i:600/3032, avg_loss:0.033, avg_succ13.345%\n",
      "epoch:2, i:800/3032, avg_loss:0.033, avg_succ13.729%\n",
      "epoch:2, i:1000/3032, avg_loss:0.033, avg_succ14.116%\n",
      "epoch:2, i:1200/3032, avg_loss:0.033, avg_succ14.609%\n",
      "epoch:2, i:1400/3032, avg_loss:0.032, avg_succ15.074%\n",
      "epoch:2, i:1600/3032, avg_loss:0.032, avg_succ15.559%\n",
      "epoch:2, i:1800/3032, avg_loss:0.032, avg_succ16.109%\n",
      "epoch:2, i:2000/3032, avg_loss:0.032, avg_succ16.597%\n",
      "epoch:2, i:2200/3032, avg_loss:0.032, avg_succ17.033%\n",
      "epoch:2, i:2400/3032, avg_loss:0.031, avg_succ17.463%\n",
      "epoch:2, i:2600/3032, avg_loss:0.031, avg_succ17.898%\n",
      "epoch:2, i:2800/3032, avg_loss:0.031, avg_succ18.348%\n",
      "epoch:2, i:3000/3032, avg_loss:0.031, avg_succ18.756%\n",
      "-----------------------------\n",
      "Epoch 2: train_pred_succ:18.805%, 388096\n",
      "Epoch 2: test_pred_succ:21.493%, 12288\n",
      "\n",
      "\n",
      "epoch:3, i:200/3032, avg_loss:0.028, avg_succ24.051%\n",
      "epoch:3, i:400/3032, avg_loss:0.028, avg_succ24.795%\n",
      "epoch:3, i:600/3032, avg_loss:0.028, avg_succ25.204%\n",
      "epoch:3, i:800/3032, avg_loss:0.028, avg_succ25.424%\n",
      "epoch:3, i:1000/3032, avg_loss:0.027, avg_succ25.575%\n",
      "epoch:3, i:1200/3032, avg_loss:0.027, avg_succ25.885%\n",
      "epoch:3, i:1400/3032, avg_loss:0.027, avg_succ26.052%\n",
      "epoch:3, i:1600/3032, avg_loss:0.027, avg_succ26.290%\n",
      "epoch:3, i:1800/3032, avg_loss:0.027, avg_succ26.628%\n",
      "epoch:3, i:2000/3032, avg_loss:0.027, avg_succ26.870%\n",
      "epoch:3, i:2200/3032, avg_loss:0.026, avg_succ27.085%\n",
      "epoch:3, i:2400/3032, avg_loss:0.026, avg_succ27.321%\n",
      "epoch:3, i:2600/3032, avg_loss:0.026, avg_succ27.575%\n",
      "epoch:3, i:2800/3032, avg_loss:0.026, avg_succ27.838%\n",
      "epoch:3, i:3000/3032, avg_loss:0.026, avg_succ28.091%\n",
      "-----------------------------\n",
      "Epoch 3: train_pred_succ:28.129%, 388096\n",
      "Epoch 3: test_pred_succ:23.763%, 12288\n",
      "\n",
      "\n",
      "epoch:4, i:200/3032, avg_loss:0.024, avg_succ30.785%\n",
      "epoch:4, i:400/3032, avg_loss:0.024, avg_succ31.518%\n",
      "epoch:4, i:600/3032, avg_loss:0.024, avg_succ32.038%\n",
      "epoch:4, i:800/3032, avg_loss:0.024, avg_succ32.142%\n",
      "epoch:4, i:1000/3032, avg_loss:0.023, avg_succ32.417%\n",
      "epoch:4, i:1200/3032, avg_loss:0.023, avg_succ32.779%\n",
      "epoch:4, i:1400/3032, avg_loss:0.023, avg_succ32.978%\n",
      "epoch:4, i:1600/3032, avg_loss:0.023, avg_succ33.226%\n",
      "epoch:4, i:1800/3032, avg_loss:0.023, avg_succ33.591%\n",
      "epoch:4, i:2000/3032, avg_loss:0.023, avg_succ33.873%\n",
      "epoch:4, i:2200/3032, avg_loss:0.022, avg_succ34.114%\n",
      "epoch:4, i:2400/3032, avg_loss:0.022, avg_succ34.393%\n",
      "epoch:4, i:2600/3032, avg_loss:0.022, avg_succ34.584%\n",
      "epoch:4, i:2800/3032, avg_loss:0.022, avg_succ34.867%\n",
      "epoch:4, i:3000/3032, avg_loss:0.022, avg_succ35.112%\n",
      "-----------------------------\n",
      "Epoch 4: train_pred_succ:35.159%, 388096\n",
      "Epoch 4: test_pred_succ:24.365%, 12288\n",
      "\n",
      "\n",
      "best_epoch:4, best_test_pred:24.365%\n",
      "1004.615s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# sgd = SGD(lr=0.1, momentum=0.9, decay=0.01) 每次迭代，lr*0.99\n",
    "\n",
    "# 两个不用调lr的优化器\n",
    "# optimizer = torch.optim.Adagrad(model.parameters()) # 还不错\n",
    "# optimizer = torch.optim.Adadelta(model.parameters()) # 也还行\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# StepLR 调度器每隔 `step_size` 个 epoch 将学习率乘以 `gamma`\n",
    "\n",
    "t1 = time.time()\n",
    "train(model=model,\n",
    "      epochs=5,\n",
    "      loss_fn=ce_loss,\n",
    "      optimizer=optimizer,\n",
    "      scheduler=scheduler,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,)\n",
    "print('{:.3f}s'.format(time.time()-t1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chapter_6 cnn架构\n",
    "\n",
    "    Epoch 4: train_pred_succ:2.424%, 388096\n",
    "    Epoch 4: test_pred_succ:2.547%, 12288\n",
    "    best_epoch:4, best_test_pred:2.547%\n",
    "    669.404s\n",
    "\n",
    "\n",
    "chapter_7 small cnn 架构\n",
    "\n",
    "    * 训练集2000\n",
    "    Epoch 4: train_pred_succ:19.276%, 388096\n",
    "    Epoch 4: test_pred_succ:18.530%, 12288\n",
    "    best_epoch:4, best_test_pred:18.530%\n",
    "    556.572s\n",
    "    结论:多cnn结构，效果马上好起来了\n",
    "\n",
    "    * 加大数据量和epoch数\n",
    "        训练集10000，epoch-10\n",
    "\n",
    "        Epoch 3: train_pred_succ:29.281%, 1961984\n",
    "        Epoch 3: test_pred_succ:24.227%, 12288\n",
    "        不知道极限是多少，但是训练过程很慢，效果还在缓慢变好\n",
    "\n",
    "7层encoder-2000games-small_cnn架构\n",
    "    \n",
    "    效果还不错，每个epoch都能涨几个点，但是过5遍epoch完全不够。\n",
    "    Epoch 4: train_pred_succ:33.546%, 388096\n",
    "    Epoch 4: test_pred_succ:22.534%, 12288\n",
    "    best_epoch:4, best_test_pred:22.534%\n",
    "    1121.978s\n",
    "\n",
    "7层encoder-2000games-resnet18架构\n",
    "\n",
    "    确实要优于普通cnn，同样计算代价下收敛更快，且不容易过拟合\n",
    "    Epoch 4: train_pred_succ:38.685%, 388096\n",
    "    Epoch 4: test_pred_succ:32.886%, 12288\n",
    "    best_epoch:4, best_test_pred:32.886%\n",
    "    1022.198s\n",
    "\n",
    "7层encoder-2000games-large_cnn架构\n",
    "    \n",
    "    如果三轮到不了书上的效果，就是学习率和数据量的问题\n",
    "    速度方面也还行，不算太慢\n",
    "\n",
    "    Epoch 4: train_pred_succ:35.159%, 388096\n",
    "    poch 4: test_pred_succ:24.365%, 12288\n",
    "    best_epoch:4, best_test_pred:24.365%\n",
    "    1004.615s\n",
    "    是不是线性层没加dropout，不如resnet那个\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 8, 9]) torch.Size([1, 64, 5, 7])\n",
      "torch.Size([1, 64, 10, 9]) torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 10, 9]) torch.Size([1, 64, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5x7\n",
    "m = nn.AdaptiveAvgPool2d((5, 7))\n",
    "input = torch.randn(1, 64, 8, 9)\n",
    "output = m(input)\n",
    "print(input.shape, output.shape)\n",
    "\n",
    "# target output size of 7x7 (square)\n",
    "m = nn.AdaptiveAvgPool2d(7)\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(input.shape, output.shape)\n",
    "\n",
    "# target output size of 10x7\n",
    "m = nn.AdaptiveAvgPool2d((None, 7))\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(input.shape, output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnnsmall\n",
    "    log1 torch.Size([1, 48, 17, 17])\n",
    "    log2 torch.Size([1, 32, 15, 15])\n",
    "    log3 torch.Size([1, 32, 13, 13])\n",
    "    log4 torch.Size([1, 32, 11, 11])\n",
    "    log5-flatten torch.Size([1, 3872])\n",
    "    log6-fc1 torch.Size([1, 722])\n",
    "    log7-fc3 torch.Size([1, 361])\n",
    "    torch.Size([1, 361])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 361])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "model = CnnSmall(encoder_planes=1)\n",
    "# print(model)\n",
    "\n",
    "input_data = torch.randn(64, 1, 19, 19)  # 假设输入为 3 通道，224x224 大小的图像\n",
    "output = model(input_data)\n",
    "print(output.size()) # expansion 直接就是分类模型了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
