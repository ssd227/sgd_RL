{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵 和 CE loss 解惑\n",
    "\n",
    "Cross-entropy [[wiki]](https://en.wikipedia.org/wiki/Cross-entropy)</br>\n",
    "```In information theory, the cross-entropy between two probability distributions p and  q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution displaystyle p.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原信息论公式实现\n",
    "def cross_entropy(p, q):\n",
    "    '''\n",
    "    p: the true distribution p\n",
    "    q: an estimated probability distribution q\n",
    "    '''\n",
    "    return torch.sum(p * torch.log(q)) / p.shape[0]\n",
    "\n",
    "def ce_loss(p, logits):\n",
    "    q = nn.Softmax()(logits)\n",
    "    return -1 * cross_entropy(p,q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def forward(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        y = logits - torch.max(logits, dim=1, keepdim=True)[0] # 防止exp(x)数值溢出  [B, 1]\n",
    "\n",
    "        lse = torch.log(torch.sum(torch.exp(y), dim=1, keepdim=True)) # [B,1]\n",
    "        zy =  torch.sum(y*labels, dim=1, keepdim=True) # [B,1]\n",
    "        return torch.sum(lse-zy) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewCrossEntropyLoss(nn.Module):\n",
    "    def forward(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        y = logits - torch.max(logits, dim=1, keepdim=True)[0] # 防止exp(x)数值溢出  [B, 1]\n",
    "\n",
    "        lse = torch.log(torch.sum(torch.exp(y), dim=1, keepdim=True)) # [B,1]\n",
    "        return -1 * torch.sum(labels * (y-lse)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class New2CrossEntropyLoss(nn.Module):\n",
    "    def forward(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        y = logits - torch.max(logits, dim=1, keepdim=True)[0] # 防止exp(x)数值溢出  [B, 1]\n",
    "\n",
    "        lse = torch.log(torch.sum(torch.exp(y), dim=1, keepdim=True)) # [B,1]\n",
    "        zy =  torch.sum(y*labels, dim=1, keepdim=True) # [B,1]\n",
    "        # return torch.sum(lse*labels - zy) / batch_size # 错？？好离谱啊\n",
    "        return torch.sum( torch.sum(lse*labels, dim=1, keepdim=True) - zy) / batch_size # 对\n",
    "        # return torch.sum((lse-y)*labels) / batch_size # 对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true CE loss: 1.8797      \n",
      "pytorch CE loss: 1.8797        \n",
      "my CE loss: 1.8797      \n",
      "my new 1 CE loss: 1.8797        \n",
      "my new 2 CE loss: 1.8797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "\n",
    "out_pytroch = nn.CrossEntropyLoss()(input, target)\n",
    "out_my = CrossEntropyLoss()(input, target)\n",
    "out_new = NewCrossEntropyLoss()(input, target)\n",
    "out_new2 = New2CrossEntropyLoss()(input, target)\n",
    "out_true = ce_loss(target, input)\n",
    "\n",
    "print('true CE loss: {:.4f}\\\n",
    "      \\npytorch CE loss: {:.4f}\\\n",
    "        \\nmy CE loss: {:.4f}\\\n",
    "      \\nmy new 1 CE loss: {:.4f}\\\n",
    "        \\nmy new 2 CE loss: {:.4f}\\n'.format(\n",
    "          out_true,\n",
    "          out_pytroch,\n",
    "          out_my,\n",
    "          out_new,\n",
    "          out_new2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 不是均衡值\n",
    "重复操作会压平分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5604,  1.0262, -0.5743,  0.4062]])\n",
      "tensor([[0.0415, 0.5510, 0.1112, 0.2964]])\n",
      "tensor([[0.1989, 0.3311, 0.2133, 0.2567]])\n",
      "\n",
      "softmax的压平过程\n",
      "iter-1: tensor([[0.0415, 0.5510, 0.1112, 0.2964]])\n",
      "iter-2: tensor([[0.1989, 0.3311, 0.2133, 0.2567]])\n",
      "iter-3: tensor([[0.2372, 0.2708, 0.2407, 0.2513]])\n",
      "iter-4: tensor([[0.2468, 0.2552, 0.2477, 0.2503]])\n",
      "iter-5: tensor([[0.2492, 0.2513, 0.2494, 0.2501]])\n",
      "iter-6: tensor([[0.2498, 0.2503, 0.2499, 0.2500]])\n",
      "iter-7: tensor([[0.2500, 0.2501, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 4)\n",
    "sm_x = X.softmax(dim=1)\n",
    "sm_sm_x = sm_x.softmax(dim=1)\n",
    "print(X)\n",
    "print(sm_x)\n",
    "print(sm_sm_x)\n",
    "\n",
    "print()\n",
    "print('softmax的压平过程')\n",
    "Y= X\n",
    "for i in range(7):\n",
    "    Y = Y.softmax(dim=1)\n",
    "    print('iter-{}:'.format(i+1), Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
